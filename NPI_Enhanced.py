# -*- coding: utf-8 -*-
"""NPI_Enhanced.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GUkJwUDqmDfdFEN3-IzWkCeS0DnL9sST
"""

import numpy as np
import torch
import torch.nn as nn
import torch.utils.data as data

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

class MaskedDataset(data.Dataset):
    def __init__(self, inputs, targets, mask_ratio=0.15):
        self.inputs = inputs
        self.targets = targets
        self.mask_ratio = mask_ratio
        self.n_features = inputs.shape[1]

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        x = self.inputs[idx].clone()
        y = self.targets[idx]
        n_mask = int(self.n_features * self.mask_ratio)
        mask_indices = np.random.choice(self.n_features, n_mask, replace=False)
        x_masked = x.clone()
        x_masked[mask_indices] = 0.0
        return x_masked, y

def train_with_masking(model, input_X, target_Y, batch_size=50, train_set_proportion=0.8,
                       num_epochs=100, lr=2.5e-4, l2=5e-5, mask_ratio=0.15):
    train_inputs = torch.tensor(input_X[:int(train_set_proportion * input_X.shape[0])], dtype=torch.float).to(device)
    train_targets = torch.tensor(target_Y[:int(train_set_proportion * target_Y.shape[0])], dtype=torch.float).to(device)
    test_inputs = torch.tensor(input_X[int(train_set_proportion * input_X.shape[0]):], dtype=torch.float).to(device)
    test_targets = torch.tensor(target_Y[int(train_set_proportion * target_Y.shape[0]):], dtype=torch.float).to(device)

    train_dataset = MaskedDataset(train_inputs, train_targets, mask_ratio)
    test_dataset = data.TensorDataset(test_inputs, test_targets)
    train_iter = data.DataLoader(train_dataset, batch_size, shuffle=True)
    test_iter = data.DataLoader(test_dataset, batch_size, shuffle=False)

    loss = nn.MSELoss()
    trainer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)

    train_epoch_loss, test_epoch_loss = [], []
    for _ in range(num_epochs):
        model.train()
        for X, y in train_iter:
            y_hat = model(X)
            l = loss(y_hat, y)
            trainer.zero_grad()
            l.backward()
            trainer.step()

        model.eval()
        with torch.no_grad():
            total_loss, total_num = 0, 0
            for X, y in train_iter:
                y_hat = model(X)
                l = loss(y_hat, y)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            train_epoch_loss.append(float(total_loss / total_num))

            total_loss, total_num = 0, 0
            for X, y in test_iter:
                y_hat = model(X)
                l = loss(y_hat, y)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            test_epoch_loss.append(float(total_loss / total_num))

    return model, train_epoch_loss, test_epoch_loss

def train_with_snr_weights(model, input_X, target_Y, SNR_per_region, batch_size=50,
                           train_set_proportion=0.8, num_epochs=100, lr=2.5e-4, l2=5e-5,
                           weight_strategy='log', weight_strength=0.3):
    train_inputs = torch.tensor(input_X[:int(train_set_proportion * input_X.shape[0])], dtype=torch.float).to(device)
    train_targets = torch.tensor(target_Y[:int(train_set_proportion * target_Y.shape[0])], dtype=torch.float).to(device)
    test_inputs = torch.tensor(input_X[int(train_set_proportion * input_X.shape[0]):], dtype=torch.float).to(device)
    test_targets = torch.tensor(target_Y[int(train_set_proportion * target_Y.shape[0]):], dtype=torch.float).to(device)

    snr_norm = SNR_per_region / SNR_per_region.max()

    if weight_strategy == 'linear':
        raw_weights = snr_norm
    elif weight_strategy == 'sqrt':
        raw_weights = np.sqrt(snr_norm)
    elif weight_strategy == 'log':
        raw_weights = np.log1p(snr_norm) / np.log1p(1.0)
    elif weight_strategy == 'soft':
        raw_weights = 0.5 + 0.5 * snr_norm
    else:
        raw_weights = snr_norm

    weights = (1 - weight_strength) + weight_strength * raw_weights
    weights = torch.FloatTensor(weights).to(device)

    train_dataset = data.TensorDataset(train_inputs, train_targets)
    test_dataset = data.TensorDataset(test_inputs, test_targets)
    train_iter = data.DataLoader(train_dataset, batch_size, shuffle=True)
    test_iter = data.DataLoader(test_dataset, batch_size, shuffle=False)

    trainer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)

    train_epoch_loss, test_epoch_loss = [], []
    for _ in range(num_epochs):
        model.train()
        for X, y in train_iter:
            y_hat = model(X)
            l = torch.mean(weights * (y_hat - y) ** 2)
            trainer.zero_grad()
            l.backward()
            trainer.step()

        model.eval()
        with torch.no_grad():
            total_loss, total_num = 0, 0
            for X, y in train_iter:
                y_hat = model(X)
                l = torch.mean((y_hat - y) ** 2)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            train_epoch_loss.append(float(total_loss / total_num))

            total_loss, total_num = 0, 0
            for X, y in test_iter:
                y_hat = model(X)
                l = torch.mean((y_hat - y) ** 2)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            test_epoch_loss.append(float(total_loss / total_num))

    return model, train_epoch_loss, test_epoch_loss

def train_multi_resolution(model_fine, model_coarse, signal_fine, signal_coarse,
                           merge_groups, batch_size=50, train_set_proportion=0.8,
                           num_epochs=100, lr=2.5e-4, l2=5e-5, using_steps=3):
    from NPI import multi2one

    inputs_fine, targets_fine = multi2one(signal_fine, steps=using_steps)
    inputs_coarse, targets_coarse = multi2one(signal_coarse, steps=using_steps)

    train_inputs_fine = torch.tensor(inputs_fine[:int(train_set_proportion * inputs_fine.shape[0])], dtype=torch.float).to(device)
    train_targets_fine = torch.tensor(targets_fine[:int(train_set_proportion * targets_fine.shape[0])], dtype=torch.float).to(device)
    test_inputs_fine = torch.tensor(inputs_fine[int(train_set_proportion * inputs_fine.shape[0]):], dtype=torch.float).to(device)
    test_targets_fine = torch.tensor(targets_fine[int(train_set_proportion * targets_fine.shape[0]):], dtype=torch.float).to(device)

    train_inputs_coarse = torch.tensor(inputs_coarse[:int(train_set_proportion * inputs_coarse.shape[0])], dtype=torch.float).to(device)
    train_targets_coarse = torch.tensor(targets_coarse[:int(train_set_proportion * targets_coarse.shape[0])], dtype=torch.float).to(device)
    test_inputs_coarse = torch.tensor(inputs_coarse[int(train_set_proportion * inputs_coarse.shape[0]):], dtype=torch.float).to(device)
    test_targets_coarse = torch.tensor(targets_coarse[int(train_set_proportion * targets_coarse.shape[0]):], dtype=torch.float).to(device)

    train_dataset_fine = data.TensorDataset(train_inputs_fine, train_targets_fine)
    test_dataset_fine = data.TensorDataset(test_inputs_fine, test_targets_fine)
    train_iter_fine = data.DataLoader(train_dataset_fine, batch_size, shuffle=True)
    test_iter_fine = data.DataLoader(test_dataset_fine, batch_size, shuffle=False)

    train_dataset_coarse = data.TensorDataset(train_inputs_coarse, train_targets_coarse)
    test_dataset_coarse = data.TensorDataset(test_inputs_coarse, test_targets_coarse)
    train_iter_coarse = data.DataLoader(train_dataset_coarse, batch_size, shuffle=True)
    test_iter_coarse = data.DataLoader(test_dataset_coarse, batch_size, shuffle=False)

    loss = nn.MSELoss()
    trainer_fine = torch.optim.Adam(model_fine.parameters(), lr=lr, weight_decay=l2)
    trainer_coarse = torch.optim.Adam(model_coarse.parameters(), lr=lr, weight_decay=l2)

    losses = {'train_fine': [], 'test_fine': [], 'train_coarse': [], 'test_coarse': []}

    for _ in range(num_epochs):
        model_fine.train()
        model_coarse.train()

        for (X_f, y_f), (X_c, y_c) in zip(train_iter_fine, train_iter_coarse):
            y_hat_f = model_fine(X_f)
            l_f = loss(y_hat_f, y_f)
            trainer_fine.zero_grad()
            l_f.backward()
            trainer_fine.step()

            y_hat_c = model_coarse(X_c)
            l_c = loss(y_hat_c, y_c)
            trainer_coarse.zero_grad()
            l_c.backward()
            trainer_coarse.step()

        model_fine.eval()
        model_coarse.eval()

        with torch.no_grad():
            total_loss, total_num = 0, 0
            for X, y in train_iter_fine:
                y_hat = model_fine(X)
                l = loss(y_hat, y)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            losses['train_fine'].append(float(total_loss / total_num))

            total_loss, total_num = 0, 0
            for X, y in test_iter_fine:
                y_hat = model_fine(X)
                l = loss(y_hat, y)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            losses['test_fine'].append(float(total_loss / total_num))

            total_loss, total_num = 0, 0
            for X, y in train_iter_coarse:
                y_hat = model_coarse(X)
                l = loss(y_hat, y)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            losses['train_coarse'].append(float(total_loss / total_num))

            total_loss, total_num = 0, 0
            for X, y in test_iter_coarse:
                y_hat = model_coarse(X)
                l = loss(y_hat, y)
                total_loss += l * y.shape[0]
                total_num += y.shape[0]
            losses['test_coarse'].append(float(total_loss / total_num))

    return model_fine, model_coarse, losses

def compute_confidence_weighted_EC(NPI_EC, SNR_per_region):
    confidence = SNR_per_region / SNR_per_region.max()
    weighted_EC = NPI_EC * confidence[:, np.newaxis]
    return weighted_EC


def map_coarse_to_fine_EC(EC_coarse, merge_groups, original_n):
    EC_fine = np.zeros((original_n, original_n))
    for i, group_i in enumerate(merge_groups):
        for j, group_j in enumerate(merge_groups):
            ec_value = EC_coarse[i, j]
            for src in group_i:
                for tgt in group_j:
                    EC_fine[src, tgt] = ec_value
    return EC_fine

